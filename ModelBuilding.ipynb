{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4315a868-64a8-46d5-91b7-5f52c9564e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectID (and not the Project Name) is: vrajan7-cis415-fa24a\n",
      "Bucket name is: vrajan7_data_for_gcp_labs\n",
      "Full paths to the data files:\n",
      "test_categorical: gs://vrajan7_data_for_gcp_labs/data_for_traffic/test_data_categorical.csv\n",
      "test_numerical: gs://vrajan7_data_for_gcp_labs/data_for_traffic/test_data_numerical.csv\n",
      "train_categorical: gs://vrajan7_data_for_gcp_labs/data_for_traffic/train_data_categorical.csv\n",
      "train_numerical: gs://vrajan7_data_for_gcp_labs/data_for_traffic/train_data_numerical.csv\n"
     ]
    }
   ],
   "source": [
    "project_id = 'vrajan7-cis415-fa24a'\n",
    "bucket = 'vrajan7_data_for_gcp_labs'\n",
    "path_to_data_files = \"data_for_traffic/\"\n",
    "\n",
    "# Define the file names\n",
    "test_categorical = \"test_data_categorical.csv\"\n",
    "test_numerical = \"test_data_numerical.csv\"\n",
    "train_categorical = \"train_data_categorical.csv\"\n",
    "train_numerical = \"train_data_numerical.csv\"\n",
    "\n",
    "# Create full paths for each file\n",
    "def create_full_path(file_name):\n",
    "    return f\"gs://{bucket}/{path_to_data_files}{file_name}\"\n",
    "\n",
    "full_paths = {\n",
    "    'test_categorical': create_full_path(test_categorical),\n",
    "    'test_numerical': create_full_path(test_numerical),\n",
    "    'train_categorical': create_full_path(train_categorical),\n",
    "    'train_numerical': create_full_path(train_numerical)\n",
    "}\n",
    "\n",
    "# Print configurations\n",
    "print(f\"ProjectID (and not the Project Name) is: {project_id}\")\n",
    "print(f\"Bucket name is: {bucket}\")\n",
    "print(\"Full paths to the data files:\")\n",
    "for key, path in full_paths.items():\n",
    "    print(f\"{key}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680a7c14-fa57-4824-b2d8-28c3d0f3b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'spark-3.0.0-bin-hadoop3.2.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.0.0-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/traffic/env/lib/python3.12/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q findspark\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     34\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m~/Desktop/traffic/env/lib/python3.12/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.0.0-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from pyspark.sql import SparkSession\n",
    "  pyspark_available = 'Y'\n",
    "except:\n",
    "  pyspark_available = 'N'\n",
    "\n",
    "# If PySpark is not installed, then go through all these steps\n",
    "\n",
    "if pyspark_available == 'N':\n",
    "  # Update Installer\n",
    "  !apt-get update\n",
    "\n",
    "  # Intsall Java\n",
    "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "  # install spark (change the version number if needed)\n",
    "  !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "  # unzip the spark file to the current folder\n",
    "  !tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "  # set your spark folder to your system path environment.\n",
    "  import os\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
    "\n",
    "  # install findspark using pip\n",
    "  !pip install -q findspark\n",
    "\n",
    "  import findspark\n",
    "  findspark.init()\n",
    "\n",
    "  from pyspark.sql import SparkSession\n",
    "  spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "  # To access Google Cloud Storage\n",
    "  from google.cloud import storage\n",
    "  import google.auth\n",
    "\n",
    "  !pip install gcsfs\n",
    "  import gcsfs\n",
    "\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "  credentials, default_project_id = google.auth.default()\n",
    "  !gcloud config set project {project_id}\n",
    "else:\n",
    "    # Spark / PySpark already pre-installed in the environment\n",
    "    print(\"PySpark already pre-installed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
